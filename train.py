import argparseimport osimport randomfrom tqdm import tqdmimport numpy as npfrom utils import compute_result# import compute_resultfrom torch.utils.tensorboard import SummaryWriter as sum_writerimport models.RESNETimport models.RES101import dataset.create_datasetimport torch.autogradfrom torch.autograd import Variableimport torch.backends.cudnnimport matplotlib.pyplot as pltos.environ['http_proxy'] = 'http://172.21.141.57:10084'os.environ['https_proxy'] = 'http://172.21.141.57:10084'def parse_config():    parser = argparse.ArgumentParser()    parser.add_argument("--train_mode", type=str, default='normal')    parser.add_argument("--data_mode", type=str, default='test', help='train test')    # test or train    parser.add_argument("--train", type=bool, default=False)    # false就是test    parser.add_argument("--train_description", type=str, default='baseline062200001',                        help='train_description')    parser.add_argument("--seed", type=int, default=19980206)    parser.add_argument("--network", type=str, default='resnet34')    # parser.add_argument("--network", type=str, default='resnet101')    parser.add_argument('--data', type=str, default='kon10k1000')    parser.add_argument('--split', type=int, default='1')    parser.add_argument('--ckpt_path', default='./checkpoint', type=str,                        metavar='PATH', help='path to checkpoints')    # QUANHZONG    parser.add_argument('--ckpt', default='model_ResNetbaseline062200001-00039.pt',                        type=str, help='name of the checkpoint to load')    # quanzhongming    parser.add_argument('--tensorboard_path', default='./runs/0622', type=str,                        metavar='PATH', help='path to checkpoints')    parser.add_argument("--batch_size", type=int, default=8)    parser.add_argument("--number_workers", type=int, default=4)    parser.add_argument("--pin_memory", type=bool, default=True)    parser.add_argument("--max_epoch", type=int, default=40)    # zhouqi    parser.add_argument("--lr", type=float, default=0.0001)    parser.add_argument("--decay_interval", type=int, default=10)    parser.add_argument("--decay_ratio", type=float, default=0.5)    parser.add_argument("--init", type=str, default='kaiming_norm')    return parser.parse_args()os.environ['CUDA_VISIBLE_DEVICES'] = '1'device = torch.device("cuda" if torch.cuda.is_available() else "cpu")def seed_torch(cfg):    torch.manual_seed(cfg.seed)    torch.cuda.manual_seed_all(cfg.seed)    torch.backends.cudnn.deterministic = Trueclass Trainer(object):    def __init__(self, config):        torch.manual_seed(config.seed)        self.config = config        self.train_mode = config.train_mode        self.data_mode = config.data_mode        # initialize the data_loader        if self.config.data == 'kon10k8000':            self.train_dataloader = dataset.create_dataset.kon10k_8000(self.config)        if self.config.data == 'kon10k1000':            self.train_dataloader = dataset.create_dataset.kon10k_1000(self.config)        if self.config.data == 'kon10k2000':            self.train_dataloader = dataset.create_dataset.kon10k_2000(self.config)        # if self.config.data == 'kadid10k1000':        #     self.train_dataloader = dataset.create_dataset.kadid10k_1000(i=config.round,        #                                                                  min_batch_size=config.batch_size,        #                                                                  train_mode=config.train_mode)        # if self.config.data == 'kadid10k2000':        #     self.train_dataloader = dataset.create_dataset.kadid10k_2000(i=config.round,        #                                                                  min_batch_size=config.batch_size,        #                                                                  train_mode=config.train_mode)        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")        # initialize the model        if self.config.network == 'resnet34':            print(config.network)            self.model = models.RESNET.model_ResNet(num_classes=1, init_mode=self.config.init)        # if config.network == 'resnet101':        #     self.model = models.RES101.model_ResNet(num_classes=1)        else:            raise NotImplementedError("Not supported network, need to be added!")        self.model.to(self.device)        self.model_name = type(self.model).__name__ + self.config.train_description        # print(self.model)        # try load the model        # initialize the loss function and optimizer        self.start_epoch = 0        self.max_epoch = config.max_epoch        self.loss_fn = torch.nn.MSELoss()        self.ckpt_path = config.ckpt_path        self.loss_fn.to(self.device)        self.initial_lr = config.lr        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config.lr, betas=(0.9, 0.999))        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer,                                                         last_epoch=self.start_epoch - 1,                                                         step_size=config.decay_interval,                                                         gamma=config.decay_ratio)        self.global_step = 1        runs_path = os.path.join(self.config.tensorboard_path, self.model_name+str(self.config.split))        self.logger = sum_writer(runs_path)        if not config.train:            ckpt = os.path.join(config.ckpt_path, config.ckpt)            self._load_checkpoint(ckpt=ckpt)    def fit(self):        if self.train_mode == 'normal':            for epoch in tqdm(range(self.start_epoch, self.max_epoch)):                self._train_single_epoch(epoch)                self.scheduler.step()    def _train_single_epoch(self, epoch):        # start training        # print('Adam learning rate: {:.8f}'.format(self.optimizer.param_groups[0]['lr']))        self.model.train()        for _, (x, y) in enumerate(self.train_dataloader):            x = Variable(x)            y = Variable(y)            x = x.to(self.device)            y = y.to(self.device).view(-1, 1)            self.optimizer.zero_grad()            predict_student, _ = self.model(x)            self.loss = self.loss_fn(predict_student, y.float().detach())            self.loss.backward()            self.optimizer.step()            self.logger.add_scalar(tag='sum_loss',                                   scalar_value=self.loss.item(),                                   global_step=self.global_step)            self.global_step += 1            if (epoch + 1) == self.config.max_epoch:                model_name = '{}-{:0>5d}.pt'.format(self.model_name, epoch)                model_name = os.path.join(self.ckpt_path, model_name)                self._save_checkpoint({                    'epoch': epoch,                    'state_dict': self.model.state_dict(),                    'optimizer': self.optimizer.state_dict(),                }, model_name)    def evl(self):        y_ = []        y_pred = []        self.model.eval()        if self.config.data_mode == 'test':            with torch.no_grad():                for index, (images, labels) in enumerate(self.train_dataloader):                    images = images.cuda()                    outputs, _ = self.model(images)                    y_.extend(labels)                    y_pred.extend(outputs.squeeze(dim=1).cpu())                # plt.scatter(np.array(y_pred),np.array(y_))                # plt.show()                #     if index % 10 ==9 and  index!=0:                #         RMSE, PLCC, SROCC, KROCC = compute_result.compute_metric(np.array(y_), np.array(y_pred))                #         print(index//10, PLCC, SROCC)                #         y_ = []                #         y_pred = []                RMSE, PLCC, SROCC, KROCC = compute_result.compute_metric(np.array(y_), np.array(y_pred))        return PLCC, SROCC    def _load_checkpoint(self, ckpt):        if os.path.isfile(ckpt):            print("[*] loading checkpoint '{}'".format(ckpt))            checkpoint = torch.load(ckpt)            self.start_epoch = checkpoint['epoch'] + 1            self.model.load_state_dict(checkpoint['state_dict'])            self.optimizer.load_state_dict(checkpoint['optimizer'])            if self.initial_lr is not None:                for param_group in self.optimizer.param_groups:                    param_group['initial_lr'] = self.initial_lr            print("[*] loaded checkpoint '{}' (epoch {})"                  .format(ckpt, checkpoint['epoch']))        else:            print("[!] no checkpoint found at '{}'".format(ckpt))    # save checkpoint    @staticmethod    def _save_checkpoint(state, filename='checkpoint.pth.tar'):        torch.save(state, filename)def main(cfg):    t = Trainer(cfg)    if cfg.train:        t.fit()    else:        plcc_, srocc_ = t.evl()        print('PLCC =',plcc_,' SROCC =', srocc_)if __name__ == "__main__":    config = parse_config()    # seed_torch(config)    for i in range(2, 3):        config = parse_config()        split = i + 22        config.split = split        config.ckpt_path = os.path.join(config.ckpt_path, str(config.split))        if not os.path.exists(config.ckpt_path):            os.makedirs(config.ckpt_path)        print(config.train_description)        main(config)